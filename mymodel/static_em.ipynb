{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.sparse import coo_matrix\n",
    "from torch_geometric.nn import Node2Vec\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(graph_path):\n",
    "  df = pd.read_csv(graph_path,index_col=0)\n",
    "  node_list = df.index\n",
    "  A = df.values\n",
    "  A=torch.LongTensor(A)\n",
    "  ed = coo_matrix(A)\n",
    "  edge_index = torch.LongTensor(np.array([ed.row, ed.col]))\n",
    "  return edge_index,node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 65, 65, 65],\n",
       "        [ 0,  7, 13,  ..., 56, 64, 65]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index,node_list = load_graph('../data/processed data/adj.csv')\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([  4,  12,  13,  24,  41,  42,  43,  45,  48,  50,  68,  74,  75,  79,\n",
       "        87,  88,  90, 100, 107, 113, 114, 116, 120, 125, 127, 128, 137, 140,\n",
       "       141, 142, 143, 144, 148, 151, 152, 153, 158, 161, 162, 163, 164, 166,\n",
       "       170, 186, 194, 202, 209, 211, 224, 229, 230, 231, 232, 233, 234, 236,\n",
       "       237, 238, 239, 243, 244, 246, 249, 261, 262, 263],\n",
       "      dtype='int64', name='id')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 964])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0,  ..., 65, 65, 65],\n",
       "        [ 0,  7, 13,  ..., 56, 64, 65]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 5.2307\n",
      "Epoch: 02, Loss: 2.9323\n",
      "Epoch: 03, Loss: 2.7628\n",
      "Epoch: 04, Loss: 2.7198\n",
      "Epoch: 05, Loss: 2.7105\n",
      "Epoch: 06, Loss: 2.6991\n",
      "Epoch: 07, Loss: 2.6125\n",
      "Epoch: 08, Loss: 2.6146\n",
      "Epoch: 09, Loss: 2.5998\n",
      "Epoch: 10, Loss: 2.5869\n",
      "Epoch: 11, Loss: 2.5839\n",
      "Epoch: 12, Loss: 2.5296\n",
      "Epoch: 13, Loss: 2.4984\n",
      "Epoch: 14, Loss: 2.4889\n",
      "Epoch: 15, Loss: 2.4773\n",
      "Epoch: 16, Loss: 2.4416\n",
      "Epoch: 17, Loss: 2.4012\n",
      "Epoch: 18, Loss: 2.4004\n",
      "Epoch: 19, Loss: 2.3797\n",
      "Epoch: 20, Loss: 2.3399\n",
      "Epoch: 21, Loss: 2.3292\n",
      "Epoch: 22, Loss: 2.2649\n",
      "Epoch: 23, Loss: 2.2774\n",
      "Epoch: 24, Loss: 2.2589\n",
      "Epoch: 25, Loss: 2.2249\n",
      "Epoch: 26, Loss: 2.2078\n",
      "Epoch: 27, Loss: 2.1760\n",
      "Epoch: 28, Loss: 2.1902\n",
      "Epoch: 29, Loss: 2.1827\n",
      "Epoch: 30, Loss: 2.1322\n",
      "Epoch: 31, Loss: 2.1532\n",
      "Epoch: 32, Loss: 2.1330\n",
      "Epoch: 33, Loss: 2.1177\n",
      "Epoch: 34, Loss: 2.0651\n",
      "Epoch: 35, Loss: 2.0147\n",
      "Epoch: 36, Loss: 1.9927\n",
      "Epoch: 37, Loss: 1.9652\n",
      "Epoch: 38, Loss: 1.9656\n",
      "Epoch: 39, Loss: 1.9829\n",
      "Epoch: 40, Loss: 1.9737\n",
      "Epoch: 41, Loss: 1.9705\n",
      "Epoch: 42, Loss: 1.9705\n",
      "Epoch: 43, Loss: 1.9544\n",
      "Epoch: 44, Loss: 1.9582\n",
      "Epoch: 45, Loss: 1.9427\n",
      "Epoch: 46, Loss: 1.9332\n",
      "Epoch: 47, Loss: 1.9150\n",
      "Epoch: 48, Loss: 1.8859\n",
      "Epoch: 49, Loss: 1.8872\n",
      "Epoch: 50, Loss: 1.8877\n",
      "Epoch: 51, Loss: 1.8684\n",
      "Epoch: 52, Loss: 1.8572\n",
      "Epoch: 53, Loss: 1.8507\n",
      "Epoch: 54, Loss: 1.8434\n",
      "Epoch: 55, Loss: 1.8386\n",
      "Epoch: 56, Loss: 1.8592\n",
      "Epoch: 57, Loss: 1.8454\n",
      "Epoch: 58, Loss: 1.8426\n",
      "Epoch: 59, Loss: 1.8289\n",
      "Epoch: 60, Loss: 1.8226\n",
      "Epoch: 61, Loss: 1.7792\n",
      "Epoch: 62, Loss: 1.7982\n",
      "Epoch: 63, Loss: 1.8018\n",
      "Epoch: 64, Loss: 1.7731\n",
      "Epoch: 65, Loss: 1.7892\n",
      "Epoch: 66, Loss: 1.7770\n",
      "Epoch: 67, Loss: 1.7771\n",
      "Epoch: 68, Loss: 1.7791\n",
      "Epoch: 69, Loss: 1.7613\n",
      "Epoch: 70, Loss: 1.7545\n",
      "Epoch: 71, Loss: 1.7285\n",
      "Epoch: 72, Loss: 1.7321\n",
      "Epoch: 73, Loss: 1.7388\n",
      "Epoch: 74, Loss: 1.7241\n",
      "Epoch: 75, Loss: 1.7239\n",
      "Epoch: 76, Loss: 1.7288\n",
      "Epoch: 77, Loss: 1.7287\n",
      "Epoch: 78, Loss: 1.7200\n",
      "Epoch: 79, Loss: 1.7226\n",
      "Epoch: 80, Loss: 1.7106\n",
      "Epoch: 81, Loss: 1.7268\n",
      "Epoch: 82, Loss: 1.7084\n",
      "Epoch: 83, Loss: 1.7161\n",
      "Epoch: 84, Loss: 1.7132\n",
      "Epoch: 85, Loss: 1.7175\n",
      "Epoch: 86, Loss: 1.7259\n",
      "Epoch: 87, Loss: 1.7099\n",
      "Epoch: 88, Loss: 1.7103\n",
      "Epoch: 89, Loss: 1.7396\n",
      "Epoch: 90, Loss: 1.7227\n",
      "Epoch: 91, Loss: 1.7158\n",
      "Epoch: 92, Loss: 1.7328\n",
      "Epoch: 93, Loss: 1.7074\n",
      "Epoch: 94, Loss: 1.7117\n",
      "Epoch: 95, Loss: 1.7094\n",
      "Epoch: 96, Loss: 1.7178\n",
      "Epoch: 97, Loss: 1.7132\n",
      "Epoch: 98, Loss: 1.7074\n",
      "Epoch: 99, Loss: 1.7231\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 初始化Node2Vec模型\n",
    "model = Node2Vec(data.edge_index, embedding_dim=128, walk_length=80,\n",
    "                 context_size=10, walks_per_node=10, num_negative_samples=1,\n",
    "                 p=1, q=1, sparse=True)\n",
    "model.to(device)\n",
    "\n",
    "# 训练模型\n",
    "loader = model.loader(batch_size=1, shuffle=True, num_workers=8)\n",
    "optimizer = torch.optim.SparseAdam(model.parameters(), lr=0.01)\n",
    "model.train()\n",
    "for epoch in range(1, 100):\n",
    "    total_loss = 0\n",
    "    for pos_rw, neg_rw in loader:\n",
    "        optimizer.zero_grad()\n",
    "        # loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss = torch.norm(model.loss(pos_rw.to(device), neg_rw.to(device)), p=2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print('Epoch: {:02d}, Loss: {:.4f}'.format(epoch, total_loss / len(loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = model.embedding.weight.data.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = embeddings.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.35409838, -0.61919445, -1.08318   , ..., -0.726736  ,\n",
       "        -0.5961575 ,  1.141425  ],\n",
       "       [-0.958784  , -0.68215865, -0.5997079 , ..., -0.16268732,\n",
       "        -0.2105388 , -0.5981028 ],\n",
       "       [-0.38101852, -0.73999757, -0.52766097, ...,  0.54665494,\n",
       "        -0.6742828 , -0.40396854],\n",
       "       ...,\n",
       "       [-0.17435788,  1.1435635 ,  0.54444146, ..., -0.24937537,\n",
       "         0.385152  ,  0.08164018],\n",
       "       [ 0.33032802, -0.01506186, -0.32034525, ..., -0.75602883,\n",
       "         0.12225862, -0.9323784 ],\n",
       "       [-0.552987  ,  0.11646862, -0.2547368 , ...,  0.40270752,\n",
       "        -0.37536028,  0.37689447]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>4</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>24</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>45</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>...</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>246</th>\n",
       "      <th>249</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.021099</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>-0.032321</td>\n",
       "      <td>-0.034517</td>\n",
       "      <td>-0.051844</td>\n",
       "      <td>-0.022581</td>\n",
       "      <td>0.024571</td>\n",
       "      <td>0.000413</td>\n",
       "      <td>-0.028761</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024387</td>\n",
       "      <td>-0.031913</td>\n",
       "      <td>-0.032622</td>\n",
       "      <td>-0.052771</td>\n",
       "      <td>-0.056220</td>\n",
       "      <td>-0.016827</td>\n",
       "      <td>0.013431</td>\n",
       "      <td>0.000291</td>\n",
       "      <td>-0.039281</td>\n",
       "      <td>-0.031833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.021099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>-0.048294</td>\n",
       "      <td>-0.064654</td>\n",
       "      <td>-0.051184</td>\n",
       "      <td>-0.039837</td>\n",
       "      <td>-0.004095</td>\n",
       "      <td>-0.031756</td>\n",
       "      <td>-0.046657</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042743</td>\n",
       "      <td>-0.046275</td>\n",
       "      <td>-0.043731</td>\n",
       "      <td>-0.038973</td>\n",
       "      <td>-0.040923</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>-0.052115</td>\n",
       "      <td>-0.053798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.020099</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.055276</td>\n",
       "      <td>-0.063219</td>\n",
       "      <td>-0.058232</td>\n",
       "      <td>-0.034250</td>\n",
       "      <td>0.019306</td>\n",
       "      <td>-0.027035</td>\n",
       "      <td>-0.029659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032239</td>\n",
       "      <td>-0.042424</td>\n",
       "      <td>-0.046975</td>\n",
       "      <td>-0.048963</td>\n",
       "      <td>-0.048234</td>\n",
       "      <td>-0.022360</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>-0.054695</td>\n",
       "      <td>-0.059950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.032321</td>\n",
       "      <td>-0.048294</td>\n",
       "      <td>-0.055276</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>0.016718</td>\n",
       "      <td>0.024999</td>\n",
       "      <td>-0.035912</td>\n",
       "      <td>0.003757</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023861</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>-0.003360</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>-0.001728</td>\n",
       "      <td>-0.004371</td>\n",
       "      <td>-0.026072</td>\n",
       "      <td>-0.048157</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>0.002196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.034517</td>\n",
       "      <td>-0.064654</td>\n",
       "      <td>-0.063219</td>\n",
       "      <td>0.023151</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.012386</td>\n",
       "      <td>0.030018</td>\n",
       "      <td>-0.056110</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>0.031224</td>\n",
       "      <td>0.029184</td>\n",
       "      <td>0.008678</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.026187</td>\n",
       "      <td>-0.061372</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.012164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>-0.016827</td>\n",
       "      <td>-0.029119</td>\n",
       "      <td>-0.022360</td>\n",
       "      <td>-0.004371</td>\n",
       "      <td>-0.013634</td>\n",
       "      <td>-0.039177</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>-0.005074</td>\n",
       "      <td>0.021583</td>\n",
       "      <td>0.011422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.009130</td>\n",
       "      <td>-0.047147</td>\n",
       "      <td>-0.042235</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>-0.013107</td>\n",
       "      <td>-0.022663</td>\n",
       "      <td>-0.021619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.013431</td>\n",
       "      <td>-0.014029</td>\n",
       "      <td>0.009280</td>\n",
       "      <td>-0.026072</td>\n",
       "      <td>-0.026187</td>\n",
       "      <td>-0.049239</td>\n",
       "      <td>0.014298</td>\n",
       "      <td>0.012600</td>\n",
       "      <td>0.018224</td>\n",
       "      <td>-0.002192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004575</td>\n",
       "      <td>-0.014225</td>\n",
       "      <td>-0.013615</td>\n",
       "      <td>-0.061367</td>\n",
       "      <td>-0.057292</td>\n",
       "      <td>0.026019</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>-0.018327</td>\n",
       "      <td>-0.029026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.000291</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.018956</td>\n",
       "      <td>-0.048157</td>\n",
       "      <td>-0.061372</td>\n",
       "      <td>-0.059484</td>\n",
       "      <td>-0.042092</td>\n",
       "      <td>0.019635</td>\n",
       "      <td>-0.018219</td>\n",
       "      <td>-0.034779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022934</td>\n",
       "      <td>-0.052288</td>\n",
       "      <td>-0.042521</td>\n",
       "      <td>-0.050435</td>\n",
       "      <td>-0.051735</td>\n",
       "      <td>-0.013107</td>\n",
       "      <td>0.023365</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.060004</td>\n",
       "      <td>-0.055828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-0.039281</td>\n",
       "      <td>-0.052115</td>\n",
       "      <td>-0.054695</td>\n",
       "      <td>0.013731</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>-0.003738</td>\n",
       "      <td>0.023142</td>\n",
       "      <td>-0.042285</td>\n",
       "      <td>0.018049</td>\n",
       "      <td>-0.017236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026756</td>\n",
       "      <td>0.002662</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>-0.015381</td>\n",
       "      <td>-0.019965</td>\n",
       "      <td>-0.022663</td>\n",
       "      <td>-0.018327</td>\n",
       "      <td>-0.060004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.022213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>-0.031833</td>\n",
       "      <td>-0.053798</td>\n",
       "      <td>-0.059950</td>\n",
       "      <td>0.002196</td>\n",
       "      <td>0.012164</td>\n",
       "      <td>-0.006828</td>\n",
       "      <td>0.014515</td>\n",
       "      <td>-0.044075</td>\n",
       "      <td>-0.009991</td>\n",
       "      <td>-0.012606</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012068</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>-0.032632</td>\n",
       "      <td>-0.029619</td>\n",
       "      <td>-0.021619</td>\n",
       "      <td>-0.029026</td>\n",
       "      <td>-0.055828</td>\n",
       "      <td>0.022213</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id        4         12        13        24        41        42        43   \\\n",
       "id                                                                          \n",
       "4    1.000000 -0.021099  0.002089 -0.032321 -0.034517 -0.051844 -0.022581   \n",
       "12  -0.021099  1.000000  0.020099 -0.048294 -0.064654 -0.051184 -0.039837   \n",
       "13   0.002089  0.020099  1.000000 -0.055276 -0.063219 -0.058232 -0.034250   \n",
       "24  -0.032321 -0.048294 -0.055276  1.000000  0.023151  0.016718  0.024999   \n",
       "41  -0.034517 -0.064654 -0.063219  0.023151  1.000000  0.012386  0.030018   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246 -0.016827 -0.029119 -0.022360 -0.004371 -0.013634 -0.039177  0.014783   \n",
       "249  0.013431 -0.014029  0.009280 -0.026072 -0.026187 -0.049239  0.014298   \n",
       "261  0.000291  0.007434  0.018956 -0.048157 -0.061372 -0.059484 -0.042092   \n",
       "262 -0.039281 -0.052115 -0.054695  0.013731  0.003728 -0.003738  0.023142   \n",
       "263 -0.031833 -0.053798 -0.059950  0.002196  0.012164 -0.006828  0.014515   \n",
       "\n",
       "id        45        48        50   ...       237       238       239  \\\n",
       "id                                 ...                                 \n",
       "4    0.024571  0.000413 -0.028761  ... -0.024387 -0.031913 -0.032622   \n",
       "12  -0.004095 -0.031756 -0.046657  ... -0.042743 -0.046275 -0.043731   \n",
       "13   0.019306 -0.027035 -0.029659  ... -0.032239 -0.042424 -0.046975   \n",
       "24  -0.035912  0.003757  0.000619  ...  0.023861  0.003140 -0.003360   \n",
       "41  -0.056110  0.005061  0.003503  ...  0.026243  0.031224  0.029184   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "246 -0.005074  0.021583  0.011422  ...  0.007555  0.001493  0.009130   \n",
       "249  0.012600  0.018224 -0.002192  ...  0.004575 -0.014225 -0.013615   \n",
       "261  0.019635 -0.018219 -0.034779  ... -0.022934 -0.052288 -0.042521   \n",
       "262 -0.042285  0.018049 -0.017236  ...  0.026756  0.002662  0.002758   \n",
       "263 -0.044075 -0.009991 -0.012606  ...  0.012068 -0.001862  0.007226   \n",
       "\n",
       "id        243       244       246       249       261       262       263  \n",
       "id                                                                         \n",
       "4   -0.052771 -0.056220 -0.016827  0.013431  0.000291 -0.039281 -0.031833  \n",
       "12  -0.038973 -0.040923 -0.029119 -0.014029  0.007434 -0.052115 -0.053798  \n",
       "13  -0.048963 -0.048234 -0.022360  0.009280  0.018956 -0.054695 -0.059950  \n",
       "24   0.001655 -0.001728 -0.004371 -0.026072 -0.048157  0.013731  0.002196  \n",
       "41   0.008678  0.005845 -0.013634 -0.026187 -0.061372  0.003728  0.012164  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "246 -0.047147 -0.042235  1.000000  0.026019 -0.013107 -0.022663 -0.021619  \n",
       "249 -0.061367 -0.057292  0.026019  1.000000  0.023365 -0.018327 -0.029026  \n",
       "261 -0.050435 -0.051735 -0.013107  0.023365  1.000000 -0.060004 -0.055828  \n",
       "262 -0.015381 -0.019965 -0.022663 -0.018327 -0.060004  1.000000  0.022213  \n",
       "263 -0.032632 -0.029619 -0.021619 -0.029026 -0.055828  0.022213  1.000000  \n",
       "\n",
       "[66 rows x 66 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 计算余弦相似度矩阵\n",
    "similarity_matrix = cosine_similarity(zz,zz)\n",
    "\n",
    "# 将相似度矩阵转换为 DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=node_list, columns=node_list)\n",
    "\n",
    "# 打印生成的 DataFrame\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "202    1.000000\n",
       "148    0.373624\n",
       "140    0.020918\n",
       "263    0.013083\n",
       "229    0.010762\n",
       "141    0.009998\n",
       "262    0.008449\n",
       "162    0.005155\n",
       "236    0.005141\n",
       "237    0.004470\n",
       "Name: 202, dtype: float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_202 = similarity_df[202].sort_values(ascending=False)\n",
    "x_202.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 964])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/dissertation/lib/python3.11/site-packages/torch_geometric/data/storage.py:304: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'edge_index'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 4.3501\n",
      "Epoch [2/100], Loss: 2.8067\n",
      "Epoch [3/100], Loss: 3.7738\n",
      "Epoch [4/100], Loss: 3.8037\n",
      "Epoch [5/100], Loss: 4.2475\n",
      "Epoch [6/100], Loss: 2.4245\n",
      "Epoch [7/100], Loss: 2.9730\n",
      "Epoch [8/100], Loss: 2.2537\n",
      "Epoch [9/100], Loss: 1.8963\n",
      "Epoch [10/100], Loss: 2.8573\n",
      "Epoch [11/100], Loss: 2.6687\n",
      "Epoch [12/100], Loss: 1.7681\n",
      "Epoch [13/100], Loss: 1.5913\n",
      "Epoch [14/100], Loss: 1.5176\n",
      "Epoch [15/100], Loss: 2.2416\n",
      "Epoch [16/100], Loss: 2.5784\n",
      "Epoch [17/100], Loss: 1.8521\n",
      "Epoch [18/100], Loss: 1.8669\n",
      "Epoch [19/100], Loss: 1.1048\n",
      "Epoch [20/100], Loss: 1.2107\n",
      "Epoch [21/100], Loss: 1.3495\n",
      "Epoch [22/100], Loss: 1.2848\n",
      "Epoch [23/100], Loss: 0.8335\n",
      "Epoch [24/100], Loss: 0.7571\n",
      "Epoch [25/100], Loss: 0.7786\n",
      "Epoch [26/100], Loss: 0.8695\n",
      "Epoch [27/100], Loss: 0.6369\n",
      "Epoch [28/100], Loss: 0.6634\n",
      "Epoch [29/100], Loss: 0.5770\n",
      "Epoch [30/100], Loss: 0.4395\n",
      "Epoch [31/100], Loss: 0.3734\n",
      "Epoch [32/100], Loss: 0.3710\n",
      "Epoch [33/100], Loss: 0.2502\n",
      "Epoch [34/100], Loss: 0.3174\n",
      "Epoch [35/100], Loss: 0.2628\n",
      "Epoch [36/100], Loss: 0.2954\n",
      "Epoch [37/100], Loss: 0.2199\n",
      "Epoch [38/100], Loss: 0.2849\n",
      "Epoch [39/100], Loss: 0.5162\n",
      "Epoch [40/100], Loss: 0.3181\n",
      "Epoch [41/100], Loss: 0.3729\n",
      "Epoch [42/100], Loss: 0.2320\n",
      "Epoch [43/100], Loss: 0.3524\n",
      "Epoch [44/100], Loss: 0.3465\n",
      "Epoch [45/100], Loss: 0.3414\n",
      "Epoch [46/100], Loss: 0.2765\n",
      "Epoch [47/100], Loss: 0.1964\n",
      "Epoch [48/100], Loss: 0.2070\n",
      "Epoch [49/100], Loss: 0.2352\n",
      "Epoch [50/100], Loss: 0.2108\n",
      "Epoch [51/100], Loss: 0.2206\n",
      "Epoch [52/100], Loss: 0.1963\n",
      "Epoch [53/100], Loss: 0.1536\n",
      "Epoch [54/100], Loss: 0.1816\n",
      "Epoch [55/100], Loss: 0.1460\n",
      "Epoch [56/100], Loss: 0.1489\n",
      "Epoch [57/100], Loss: 0.1315\n",
      "Epoch [58/100], Loss: 0.1061\n",
      "Epoch [59/100], Loss: 0.1780\n",
      "Epoch [60/100], Loss: 0.1744\n",
      "Epoch [61/100], Loss: 0.1423\n",
      "Epoch [62/100], Loss: 0.1140\n",
      "Epoch [63/100], Loss: 0.1445\n",
      "Epoch [64/100], Loss: 0.1119\n",
      "Epoch [65/100], Loss: 0.1218\n",
      "Epoch [66/100], Loss: 0.1186\n",
      "Epoch [67/100], Loss: 0.1356\n",
      "Epoch [68/100], Loss: 0.1083\n",
      "Epoch [69/100], Loss: 0.1313\n",
      "Epoch [70/100], Loss: 0.1177\n",
      "Epoch [71/100], Loss: 0.1114\n",
      "Epoch [72/100], Loss: 0.1393\n",
      "Epoch [73/100], Loss: 0.1048\n",
      "Epoch [74/100], Loss: 0.1011\n",
      "Epoch [75/100], Loss: 0.1032\n",
      "Epoch [76/100], Loss: 0.1013\n",
      "Epoch [77/100], Loss: 0.1073\n",
      "Epoch [78/100], Loss: 0.0880\n",
      "Epoch [79/100], Loss: 0.0858\n",
      "Epoch [80/100], Loss: 0.0809\n",
      "Epoch [81/100], Loss: 0.0747\n",
      "Epoch [82/100], Loss: 0.0730\n",
      "Epoch [83/100], Loss: 0.0859\n",
      "Epoch [84/100], Loss: 0.0729\n",
      "Epoch [85/100], Loss: 0.0716\n",
      "Epoch [86/100], Loss: 0.0683\n",
      "Epoch [87/100], Loss: 0.0775\n",
      "Epoch [88/100], Loss: 0.0571\n",
      "Epoch [89/100], Loss: 0.0750\n",
      "Epoch [90/100], Loss: 0.0693\n",
      "Epoch [91/100], Loss: 0.0741\n",
      "Epoch [92/100], Loss: 0.0664\n",
      "Epoch [93/100], Loss: 0.0745\n",
      "Epoch [94/100], Loss: 0.0630\n",
      "Epoch [95/100], Loss: 0.0675\n",
      "Epoch [96/100], Loss: 0.0730\n",
      "Epoch [97/100], Loss: 0.0721\n",
      "Epoch [98/100], Loss: 0.0599\n",
      "Epoch [99/100], Loss: 0.0914\n",
      "Epoch [100/100], Loss: 0.1078\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.data import Data\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 定义 GAT 模型\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_heads):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, out_channels, heads=num_heads)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# 创建 GAT 模型实例\n",
    "in_channels = 1  # 输入特征的维度（这里只是一个示例）\n",
    "out_channels = 64  # 输出特征的维度\n",
    "num_heads = 2  # 注意力头的数量\n",
    "model = GATModel(in_channels, out_channels, num_heads)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练节点嵌入\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = torch.randn(data.num_nodes, in_channels)  # 随机初始化节点特征\n",
    "    embeddings = model(x, data.edge_index)\n",
    "    loss = torch.norm(embeddings, p=2)  # 正则化项，可以根据需要调整\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 提取节点嵌入\n",
    "node_embeddings = embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6144e-03, -2.1190e-04,  5.0573e-04,  ...,  1.1055e-03,\n",
       "          3.9618e-05, -2.1765e-04],\n",
       "        [ 2.3835e-03, -7.6595e-04,  1.2127e-03,  ...,  6.0867e-03,\n",
       "          2.0788e-04,  2.7742e-03],\n",
       "        [ 1.9169e-03, -4.2981e-04,  7.8377e-04,  ...,  3.0525e-03,\n",
       "          1.0539e-04,  9.5177e-04],\n",
       "        ...,\n",
       "        [ 1.9165e-03, -4.2956e-04,  7.8345e-04,  ...,  3.0532e-03,\n",
       "          1.0541e-04,  9.5221e-04],\n",
       "        [ 1.7421e-03, -3.0395e-04,  6.2318e-04,  ...,  1.9354e-03,\n",
       "          6.7650e-05,  2.8079e-04],\n",
       "        [ 1.7423e-03, -3.0409e-04,  6.2336e-04,  ...,  1.9349e-03,\n",
       "          6.7636e-05,  2.8053e-04]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings = node_embeddings.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>id</th>\n",
       "      <th>4</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>24</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>45</th>\n",
       "      <th>48</th>\n",
       "      <th>50</th>\n",
       "      <th>...</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>246</th>\n",
       "      <th>249</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.581424</td>\n",
       "      <td>0.788388</td>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.986958</td>\n",
       "      <td>0.999945</td>\n",
       "      <td>0.979693</td>\n",
       "      <td>0.880062</td>\n",
       "      <td>0.965461</td>\n",
       "      <td>0.478708</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963842</td>\n",
       "      <td>0.919966</td>\n",
       "      <td>0.949896</td>\n",
       "      <td>0.645437</td>\n",
       "      <td>0.689049</td>\n",
       "      <td>0.903453</td>\n",
       "      <td>0.929914</td>\n",
       "      <td>0.788501</td>\n",
       "      <td>0.930898</td>\n",
       "      <td>0.930839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.581424</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958894</td>\n",
       "      <td>0.849295</td>\n",
       "      <td>0.704803</td>\n",
       "      <td>0.572940</td>\n",
       "      <td>0.732742</td>\n",
       "      <td>0.898034</td>\n",
       "      <td>0.349366</td>\n",
       "      <td>-0.435985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.777202</td>\n",
       "      <td>0.853819</td>\n",
       "      <td>0.806592</td>\n",
       "      <td>-0.246164</td>\n",
       "      <td>-0.188997</td>\n",
       "      <td>0.176510</td>\n",
       "      <td>0.839895</td>\n",
       "      <td>0.958843</td>\n",
       "      <td>0.838438</td>\n",
       "      <td>0.838525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.788388</td>\n",
       "      <td>0.958894</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>0.877125</td>\n",
       "      <td>0.781951</td>\n",
       "      <td>0.895717</td>\n",
       "      <td>0.985953</td>\n",
       "      <td>0.600877</td>\n",
       "      <td>-0.162703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923805</td>\n",
       "      <td>0.966434</td>\n",
       "      <td>0.941163</td>\n",
       "      <td>0.038974</td>\n",
       "      <td>0.097411</td>\n",
       "      <td>0.448552</td>\n",
       "      <td>0.959376</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958618</td>\n",
       "      <td>0.958665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.923314</td>\n",
       "      <td>0.849295</td>\n",
       "      <td>0.964181</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.973093</td>\n",
       "      <td>0.919276</td>\n",
       "      <td>0.981566</td>\n",
       "      <td>0.994938</td>\n",
       "      <td>0.791368</td>\n",
       "      <td>0.104823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992267</td>\n",
       "      <td>0.999963</td>\n",
       "      <td>0.997092</td>\n",
       "      <td>0.302608</td>\n",
       "      <td>0.357890</td>\n",
       "      <td>0.669539</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.999797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.986958</td>\n",
       "      <td>0.704803</td>\n",
       "      <td>0.877125</td>\n",
       "      <td>0.973093</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.985235</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0.945019</td>\n",
       "      <td>0.910935</td>\n",
       "      <td>0.331146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994167</td>\n",
       "      <td>0.971068</td>\n",
       "      <td>0.987823</td>\n",
       "      <td>0.514076</td>\n",
       "      <td>0.563412</td>\n",
       "      <td>0.822669</td>\n",
       "      <td>0.976988</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>0.977556</td>\n",
       "      <td>0.977521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>0.903453</td>\n",
       "      <td>0.176510</td>\n",
       "      <td>0.448552</td>\n",
       "      <td>0.669539</td>\n",
       "      <td>0.822669</td>\n",
       "      <td>0.907859</td>\n",
       "      <td>0.799158</td>\n",
       "      <td>0.591528</td>\n",
       "      <td>0.983941</td>\n",
       "      <td>0.808867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.756555</td>\n",
       "      <td>0.663105</td>\n",
       "      <td>0.724199</td>\n",
       "      <td>0.910559</td>\n",
       "      <td>0.933200</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.682477</td>\n",
       "      <td>0.448717</td>\n",
       "      <td>0.684433</td>\n",
       "      <td>0.684315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>0.929914</td>\n",
       "      <td>0.839895</td>\n",
       "      <td>0.959376</td>\n",
       "      <td>0.999846</td>\n",
       "      <td>0.976988</td>\n",
       "      <td>0.926045</td>\n",
       "      <td>0.984770</td>\n",
       "      <td>0.993021</td>\n",
       "      <td>0.801979</td>\n",
       "      <td>0.122266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994294</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.998276</td>\n",
       "      <td>0.319295</td>\n",
       "      <td>0.374229</td>\n",
       "      <td>0.682477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.959429</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.999996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.788501</td>\n",
       "      <td>0.958843</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.964231</td>\n",
       "      <td>0.877215</td>\n",
       "      <td>0.782066</td>\n",
       "      <td>0.895800</td>\n",
       "      <td>0.985984</td>\n",
       "      <td>0.601024</td>\n",
       "      <td>-0.162521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923877</td>\n",
       "      <td>0.966482</td>\n",
       "      <td>0.941227</td>\n",
       "      <td>0.039158</td>\n",
       "      <td>0.097594</td>\n",
       "      <td>0.448717</td>\n",
       "      <td>0.959429</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958672</td>\n",
       "      <td>0.958718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.930898</td>\n",
       "      <td>0.838438</td>\n",
       "      <td>0.958618</td>\n",
       "      <td>0.999795</td>\n",
       "      <td>0.977556</td>\n",
       "      <td>0.927052</td>\n",
       "      <td>0.985233</td>\n",
       "      <td>0.992703</td>\n",
       "      <td>0.803577</td>\n",
       "      <td>0.124925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994576</td>\n",
       "      <td>0.999583</td>\n",
       "      <td>0.998429</td>\n",
       "      <td>0.321833</td>\n",
       "      <td>0.376713</td>\n",
       "      <td>0.684433</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.958672</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.930839</td>\n",
       "      <td>0.838525</td>\n",
       "      <td>0.958665</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>0.977521</td>\n",
       "      <td>0.926992</td>\n",
       "      <td>0.985205</td>\n",
       "      <td>0.992722</td>\n",
       "      <td>0.803480</td>\n",
       "      <td>0.124765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994558</td>\n",
       "      <td>0.999587</td>\n",
       "      <td>0.998420</td>\n",
       "      <td>0.321680</td>\n",
       "      <td>0.376564</td>\n",
       "      <td>0.684315</td>\n",
       "      <td>0.999996</td>\n",
       "      <td>0.958718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>66 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "id        4         12        13        24        41        42        43   \\\n",
       "id                                                                          \n",
       "4    1.000000  0.581424  0.788388  0.923314  0.986958  0.999945  0.979693   \n",
       "12   0.581424  1.000000  0.958894  0.849295  0.704803  0.572940  0.732742   \n",
       "13   0.788388  0.958894  1.000000  0.964181  0.877125  0.781951  0.895717   \n",
       "24   0.923314  0.849295  0.964181  1.000000  0.973093  0.919276  0.981566   \n",
       "41   0.986958  0.704803  0.877125  0.973093  1.000000  0.985235  0.999192   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "246  0.903453  0.176510  0.448552  0.669539  0.822669  0.907859  0.799158   \n",
       "249  0.929914  0.839895  0.959376  0.999846  0.976988  0.926045  0.984770   \n",
       "261  0.788501  0.958843  1.000000  0.964231  0.877215  0.782066  0.895800   \n",
       "262  0.930898  0.838438  0.958618  0.999795  0.977556  0.927052  0.985233   \n",
       "263  0.930839  0.838525  0.958665  0.999797  0.977521  0.926992  0.985205   \n",
       "\n",
       "id        45        48        50   ...       237       238       239  \\\n",
       "id                                 ...                                 \n",
       "4    0.880062  0.965461  0.478708  ...  0.963842  0.919966  0.949896   \n",
       "12   0.898034  0.349366 -0.435985  ...  0.777202  0.853819  0.806592   \n",
       "13   0.985953  0.600877 -0.162703  ...  0.923805  0.966434  0.941163   \n",
       "24   0.994938  0.791368  0.104823  ...  0.992267  0.999963  0.997092   \n",
       "41   0.945019  0.910935  0.331146  ...  0.994167  0.971068  0.987823   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "246  0.591528  0.983941  0.808867  ...  0.756555  0.663105  0.724199   \n",
       "249  0.993021  0.801979  0.122266  ...  0.994294  0.999657  0.998276   \n",
       "261  0.985984  0.601024 -0.162521  ...  0.923877  0.966482  0.941227   \n",
       "262  0.992703  0.803577  0.124925  ...  0.994576  0.999583  0.998429   \n",
       "263  0.992722  0.803480  0.124765  ...  0.994558  0.999587  0.998420   \n",
       "\n",
       "id        243       244       246       249       261       262       263  \n",
       "id                                                                         \n",
       "4    0.645437  0.689049  0.903453  0.929914  0.788501  0.930898  0.930839  \n",
       "12  -0.246164 -0.188997  0.176510  0.839895  0.958843  0.838438  0.838525  \n",
       "13   0.038974  0.097411  0.448552  0.959376  1.000000  0.958618  0.958665  \n",
       "24   0.302608  0.357890  0.669539  0.999846  0.964231  0.999795  0.999797  \n",
       "41   0.514076  0.563412  0.822669  0.976988  0.877215  0.977556  0.977521  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "246  0.910559  0.933200  1.000000  0.682477  0.448717  0.684433  0.684315  \n",
       "249  0.319295  0.374229  0.682477  1.000000  0.959429  0.999996  0.999996  \n",
       "261  0.039158  0.097594  0.448717  0.959429  1.000000  0.958672  0.958718  \n",
       "262  0.321833  0.376713  0.684433  0.999996  0.958672  1.000000  1.000000  \n",
       "263  0.321680  0.376564  0.684315  0.999996  0.958718  1.000000  1.000000  \n",
       "\n",
       "[66 rows x 66 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 计算余弦相似度矩阵\n",
    "similarity_matrix = cosine_similarity(node_embeddings,node_embeddings)\n",
    "\n",
    "# 将相似度矩阵转换为 DataFrame\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=node_list, columns=node_list)\n",
    "\n",
    "# 打印生成的 DataFrame\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "202    1.000000\n",
       "100    0.999994\n",
       "114    0.999954\n",
       "68     0.999467\n",
       "142    0.998220\n",
       "42     0.997760\n",
       "4      0.997008\n",
       "166    0.993794\n",
       "161    0.982933\n",
       "48     0.982702\n",
       "158    0.978605\n",
       "74     0.978299\n",
       "41     0.971578\n",
       "230    0.969477\n",
       "153    0.963100\n",
       "148    0.963091\n",
       "43     0.961280\n",
       "125    0.960682\n",
       "75     0.959710\n",
       "163    0.953340\n",
       "162    0.951712\n",
       "232    0.948751\n",
       "116    0.943861\n",
       "237    0.940380\n",
       "186    0.939487\n",
       "236    0.934753\n",
       "246    0.933865\n",
       "239    0.922917\n",
       "151    0.921212\n",
       "90     0.917961\n",
       "152    0.914592\n",
       "79     0.901414\n",
       "262    0.899900\n",
       "263    0.899828\n",
       "249    0.898730\n",
       "24     0.890892\n",
       "234    0.887774\n",
       "113    0.887345\n",
       "238    0.886940\n",
       "229    0.884473\n",
       "164    0.858102\n",
       "233    0.851459\n",
       "45     0.840747\n",
       "141    0.820433\n",
       "120    0.807397\n",
       "211    0.802789\n",
       "140    0.800400\n",
       "107    0.796000\n",
       "209    0.793910\n",
       "224    0.790837\n",
       "170    0.776967\n",
       "231    0.771330\n",
       "144    0.757731\n",
       "128    0.752418\n",
       "244    0.742969\n",
       "261    0.738633\n",
       "13     0.738507\n",
       "127    0.702510\n",
       "243    0.702509\n",
       "137    0.701675\n",
       "Name: 202, dtype: float32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_202 = similarity_df[202].sort_values(ascending=False)\n",
    "x_202.head(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dissertation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
